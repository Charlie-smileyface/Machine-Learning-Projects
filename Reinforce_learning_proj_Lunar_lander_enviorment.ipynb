{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222ac98b-17f8-4f74-b3f4-5c6bb5b1ac94",
   "metadata": {},
   "source": [
    "# Reinforcement learning project\n",
    "\n",
    "Contact:\n",
    "Elie KADOCHE,\n",
    "eliekadoche78@gmail.com.\n",
    "\n",
    "This is a reinforcement learning project on deep Q-learning and policy gradient methods.\n",
    "More explanations will be given in the board during the practical sessions.\n",
    "Do not hesitate to ask for help if you need to.\n",
    "\n",
    "In `dummy_deep.py`, you will find a minimal example of building and training a neural network with PyTorch.\n",
    "The model learns to approximate the square root of a number.\n",
    "This example is designed to help you get familiar with PyTorch syntax and workflow, which you'll use throughout the lab.\n",
    "\n",
    "You will write your answers, thoughts and approaches for each question.\n",
    "Even if you do some mistakes, you can describe them and explain how you corrected them.\n",
    "Each time there is a `# ---> TODO:` comment, you need to add some code.\n",
    "Be mindful of the code quality and explain your code with specific comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74378ac3-43e7-4fc7-a70d-6ccf2e49a1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\24991\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\24991\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\24991\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\24991\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\24991\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\24991\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\24991\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: swig in c:\\users\\24991\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\24991\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\24991\\anaconda3\\lib\\site-packages (from gymnasium[box2d]) (4.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install swig\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31248767-7375-4227-adf4-711fc4b7a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bd312-8bc6-4576-a485-b9ddf885df84",
   "metadata": {},
   "source": [
    "## 1) Lunar Lander environment\n",
    "\n",
    "Your objective is to understand how the Lunar Lander environment works and what is the problem we want to solve.\n",
    "You need to write what are the states, the rewards and the actions.\n",
    "Write down the Markov decision process associated to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f9cfa-dfad-4003-b597-1bedae0aa4e4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Lunar Lander Environment\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective in the Lunar Lander environment is to train an agent to safely land a lunar module on a designated landing pad. The landing pad is marked by two flags. The agent controls the lander's thrusters to navigate through a 2D space, combat gravity, and achieve a soft landing.\n",
    "\n",
    "A successful landing requires the lander to come to a complete stop on the landing pad. Crashing the lander or landing outside the designated area results in failure. The agent must learn to manage its fuel consumption, as firing thrusters incurs a cost.\n",
    "\n",
    "### Markov Decision Process (MDP)\n",
    "\n",
    "The Lunar Lander problem can be formally described as a Markov Decision Process (MDP).\n",
    "\n",
    "*   **S**: A set of states.\n",
    "*   **A**: A set of actions.\n",
    "*   **P**: The state transition probability function (transition kernel), `P(s' | s, a)`.\n",
    "*   **R**: The reward function, `R(s,a)`.\n",
    "*   **γ**: The discount factor.\n",
    "\n",
    "#### **State Space (S)**\n",
    "\n",
    "The state is a continuous **8-dimensional vector** that provides all the necessary information about the lander's current situation. The components of the state vector are:\n",
    "\n",
    "1.  **Horizontal coordinate (x)**: The lander's position on the x-axis.\n",
    "2.  **Vertical coordinate (y)**: The lander's position on the y-axis.\n",
    "3.  **Horizontal velocity (vx)**: The lander's speed along the x-axis.\n",
    "4.  **Vertical velocity (vy)**: The lander's speed along the y-axis.\n",
    "5.  **Angle (θ)**: The lander's angle in radians.\n",
    "6.  **Angular velocity (vθ)**: The rate at which the lander's angle is changing.\n",
    "7.  **Left leg contact (boolean)**: `1` if the left leg is in contact with the ground, `0` otherwise.\n",
    "8.  **Right leg contact (boolean)**: `1` if the right leg is in contact with the ground, `0` otherwise.\n",
    "\n",
    "#### **Action Space (A)**\n",
    "\n",
    "The code specifies `continuous=False`, so we are using the **discrete action space**. There are 4 possible discrete actions the agent can take at each time step:\n",
    "\n",
    "*   **0**: `do nothing` - The lander drifts according to gravity and its current momentum.\n",
    "*   **1**: `fire left orientation engine` - Applies a force to rotate the lander counter-clockwise.\n",
    "*   **2**: `fire main engine` - Applies an upward force to counteract gravity and slow the lander's descent.\n",
    "*   **3**: `fire right orientation engine` - Applies a force to rotate the lander clockwise.\n",
    "\n",
    "#### **Reward Function (R)**\n",
    "\n",
    "The reward function is shaped to guide the agent towards a successful landing. The total reward for an episode is the sum of rewards received at each step.\n",
    "\n",
    "*   **Moving towards the pad**: A positive reward is given for moving from a point far from the landing pad to a point closer to it. Conversely, moving away results in a penalty.\n",
    "*   **Successful Landing**: If the lander comes to rest on the landing pad, it receives a large reward of **+100 points**.\n",
    "*   **Crashing**: If the lander crashes, it receives a large penalty of **-100 points**.\n",
    "*   **Leg Contact**: Each leg that touches the ground gives a reward of **+10 points**.\n",
    "*   **Fuel Cost**: Firing the main engine costs a small amount of fuel, resulting in a small negative reward (e.g., -0.3 points per frame). Firing the side engines also has a smaller cost (e.g., -0.03 points per frame).\n",
    "\n",
    "The problem is considered \"solved\" when the agent consistently achieves an average total reward of 200 points over 100 consecutive episodes.\n",
    "\n",
    "#### **Transition Kernel (P)**\n",
    "\n",
    "The state transition function `P(s' | s, a)` defines the probability of moving to state `s'` after taking action `a` in state `s`. In this environment, the transitions are **deterministic** and governed by the **Box2D physics engine**. Given a state and an action, the physics engine calculates the next state based on forces like gravity, thruster power, and ground contact dynamics. While the outcome is deterministic, the underlying physics equations are complex and are handled internally by the environment.\n",
    "\n",
    "#### **Discount Factor (γ)**\n",
    "\n",
    "The discount factor, represented as `discount_factor` in the code, is a value between 0 and 1. It determines the importance of future rewards. In other words, today's reward is more important then future rewards.\n",
    "\n",
    "#### **Episode Termination**\n",
    "\n",
    "An episode ends (`terminated`) under one of the following conditions:\n",
    "\n",
    "1.  The lander crashes (e.g., its body hits the ground or it moves too fast).\n",
    "2.  The lander comes to a complete rest on the landing pad.\n",
    "3.  The lander flies out of the screen boundaries.\n",
    "\n",
    "Additionally, an episode can be `truncated` if it exceeds a maximum number of steps (typically 1000 steps), preventing it from running indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa220de8-50e1-4e11-bb72-48bf20c3bf68",
   "metadata": {},
   "source": [
    "### Based on the provided code, a single step within the `while` loop represents one full cycle of the Markov Decision Process. Here is a detailed breakdown of this dynamic process:\n",
    "\n",
    "1.  **Observe the State (`S_t`)**: At the beginning of each loop iteration, the agent is in a specific state, `S_t`. In the code, this is represented by the `observation` variable.\n",
    "\n",
    "2.  **Select an Action (`A_t`)**: The agent must now choose an action to take. In this specific \"random policy\" example, the agent completely ignores the current state `S_t`. Instead, it uses `env.action_space.sample()` to select an action `A_t` uniformly at random from the four possible discrete actions. This is the simplest possible policy, where every action is equally likely, regardless of the situation.\n",
    "\n",
    "3.  **Interact with the Environment (`env.step(A_t)`)**: The chosen action `A_t` is sent to the environment. The `env.step(action)` function is the core of the interaction. The environment takes the current state `S_t` and the agent's action `A_t` and performs two critical calculations based on its internal physics engine (the transition dynamics):\n",
    "    *   **State Transition (`P(S_{t+1} | S_t, A_t)`)**: It computes the next state of the lander, `S_{t+1}`. For example, if the action was \"fire main engine,\" the lander's vertical velocity will decrease and its vertical position will increase.\n",
    "    *   **Calculate Reward (`R_{t+1}`)**: It calculates the immediate reward, `R_{t+1}`, that results from this transition from `S_t` to `S_{t+1}` by taking action `A_t`.\n",
    "\n",
    "4.  **Receive New State and Reward**: The `env.step()` function returns the results to the agent. The `observation` variable is updated with the new state `S_{t+1}`. The agent also receives the scalar `reward` value (`R_{t+1}`) and boolean flags (`terminated`, `truncated`) that indicate if the episode has ended.\n",
    "\n",
    "5.  **Update and Repeat**: The agent updates its `total_reward` by adding the received reward. The `finished` flag is updated. The loop then repeats, with the new `observation` (`S_{t+1}`) now serving as the current state for the next decision-making cycle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849a743a-175b-4446-8f32-60846c55ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward = -100.72087786830204\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run a random policy.\"\"\"\n",
    "\n",
    "# Create and reset environment\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "# here the observation is the state of the environment\n",
    "obsevation, info = env.reset(seed=None)\n",
    "total_reward = 0.0\n",
    "\n",
    "# While the episode is not finished\n",
    "finished = False\n",
    "while not finished:\n",
    "\n",
    "    # Select a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # One step forward\n",
    "    # env.step(action): go to the next step based on the action\n",
    "    obsevation, reward, terminated, truncated, info = env.step(action)\n",
    "    finished = terminated or truncated\n",
    "\n",
    "    # Eventually render the environment (render mode should be \"human\")\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "# Print reward\n",
    "print(\"total_reward = {}\".format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d39fca-a26f-4e1f-9f58-25726173ac36",
   "metadata": {},
   "source": [
    "## 2) Deep neural Q-network\n",
    "\n",
    "We aim to build a deep Q-network $ Q_\\theta(s, a) $, which estimates the Q-value for each action $ a $ given a state $ s $.\n",
    "This network is parameterized by weights $ \\theta $ and replaces the classical Q-table used in tabular methods.\n",
    "\n",
    "During learning, the network is updated to minimize the temporal difference (TD) error: $\\delta = r + \\gamma \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a)$.\n",
    "This leads to the Q-learning update rule: $Q_\\theta(s, a) \\leftarrow Q_\\theta(s, a) + \\alpha \\, \\delta$.\n",
    "In practice, we minimize the squared TD error using gradient descent.\n",
    "\n",
    "Below are 3 code samples.\n",
    "- A class implementing the Q-network. You must specify `input_size` and `nb_actions`.\n",
    "- A script to train the Q-network. Complete the missing parts of the code.\n",
    "- A script to test the Q-network. Determine how to select an action from the Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b271d32a-fb72-4f64-8690-c463771be3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Deep neural Q-network.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        # The input is the state of the environment. The Lunar Lander state\n",
    "        # is an 8-dimensional vector (x, y, vx, vy, angle, angular_velocity, leg1_contact, leg2_contact).\n",
    "        input_size = 8\n",
    "\n",
    "        # The output is a Q-value for each possible action. The Lunar Lander\n",
    "        # has 4 discrete actions (do nothing, fire left, fire main, fire right).\n",
    "        nb_actions = 4\n",
    "\n",
    "        # Layers\n",
    "        self.layer_a = nn.Linear(input_size, 128)\n",
    "        self.layer_b = nn.Linear(128, 128)\n",
    "        self.layer_c = nn.Linear(128, nb_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward.\"\"\"\n",
    "        x = F.relu(self.layer_a(x))\n",
    "        x = F.relu(self.layer_b(x))\n",
    "        q_values = self.layer_c(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be960e66-3942-4671-b45f-4dc5139c51c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5 - last reward: -291.76\n",
      "iteration 10 - last reward: -97.84\n",
      "iteration 15 - last reward: -82.56\n",
      "iteration 20 - last reward: -195.42\n",
      "iteration 25 - last reward: -426.02\n",
      "iteration 30 - last reward: -79.98\n",
      "iteration 35 - last reward: -226.26\n",
      "iteration 40 - last reward: -43.78\n",
      "iteration 45 - last reward: -381.63\n",
      "iteration 50 - last reward: -232.14\n",
      "iteration 55 - last reward: -269.84\n",
      "iteration 60 - last reward: 6.27\n",
      "iteration 65 - last reward: -110.84\n",
      "iteration 70 - last reward: -157.07\n",
      "iteration 75 - last reward: -31.79\n",
      "iteration 80 - last reward: -107.06\n",
      "iteration 85 - last reward: -242.10\n",
      "iteration 90 - last reward: -367.54\n",
      "iteration 95 - last reward: -98.51\n",
      "iteration 100 - last reward: -53.09\n",
      "iteration 105 - last reward: -138.26\n",
      "iteration 110 - last reward: -338.06\n",
      "iteration 115 - last reward: -122.79\n",
      "iteration 120 - last reward: -129.79\n",
      "iteration 125 - last reward: -151.89\n",
      "iteration 130 - last reward: -99.41\n",
      "iteration 135 - last reward: -206.43\n",
      "iteration 140 - last reward: -97.77\n",
      "iteration 145 - last reward: -92.65\n",
      "iteration 150 - last reward: -308.85\n",
      "iteration 155 - last reward: -106.21\n",
      "iteration 160 - last reward: -90.16\n",
      "iteration 165 - last reward: -167.61\n",
      "iteration 170 - last reward: -189.83\n",
      "iteration 175 - last reward: -413.72\n",
      "iteration 180 - last reward: -188.16\n",
      "iteration 185 - last reward: -339.27\n",
      "iteration 190 - last reward: -29.63\n",
      "iteration 195 - last reward: -109.35\n",
      "iteration 200 - last reward: -68.64\n",
      "iteration 205 - last reward: -181.06\n",
      "iteration 210 - last reward: -194.65\n",
      "iteration 215 - last reward: -218.49\n",
      "iteration 220 - last reward: -213.18\n",
      "iteration 225 - last reward: -122.95\n",
      "iteration 230 - last reward: -120.76\n",
      "iteration 235 - last reward: -373.46\n",
      "iteration 240 - last reward: -133.58\n",
      "iteration 245 - last reward: -275.70\n",
      "iteration 250 - last reward: -234.21\n",
      "iteration 255 - last reward: -89.58\n",
      "iteration 260 - last reward: -173.55\n",
      "iteration 265 - last reward: -188.71\n",
      "iteration 270 - last reward: -90.80\n",
      "iteration 275 - last reward: -235.86\n",
      "iteration 280 - last reward: -385.12\n",
      "iteration 285 - last reward: -151.32\n",
      "iteration 290 - last reward: -118.78\n",
      "iteration 295 - last reward: -210.34\n",
      "iteration 300 - last reward: -91.99\n",
      "iteration 305 - last reward: -263.38\n",
      "iteration 310 - last reward: -88.73\n",
      "iteration 315 - last reward: -139.32\n",
      "iteration 320 - last reward: -287.70\n",
      "iteration 325 - last reward: -118.29\n",
      "iteration 330 - last reward: -113.32\n",
      "iteration 335 - last reward: -177.09\n",
      "iteration 340 - last reward: -131.67\n",
      "iteration 345 - last reward: -260.64\n",
      "iteration 350 - last reward: -75.01\n",
      "iteration 355 - last reward: -98.31\n",
      "iteration 360 - last reward: -59.70\n",
      "iteration 365 - last reward: -129.35\n",
      "iteration 370 - last reward: -153.07\n",
      "iteration 375 - last reward: -90.28\n",
      "iteration 380 - last reward: -98.31\n",
      "iteration 385 - last reward: -146.93\n",
      "iteration 390 - last reward: -130.49\n",
      "iteration 395 - last reward: -111.98\n",
      "iteration 400 - last reward: -109.44\n",
      "iteration 405 - last reward: -101.18\n",
      "iteration 410 - last reward: -64.95\n",
      "iteration 415 - last reward: 68.00\n",
      "iteration 420 - last reward: -126.04\n",
      "iteration 425 - last reward: -290.68\n",
      "iteration 430 - last reward: -197.75\n",
      "iteration 435 - last reward: -95.46\n",
      "iteration 440 - last reward: -1.09\n",
      "iteration 445 - last reward: -86.21\n",
      "iteration 450 - last reward: -61.92\n",
      "iteration 455 - last reward: -125.65\n",
      "iteration 460 - last reward: -133.27\n",
      "iteration 465 - last reward: -26.03\n",
      "iteration 470 - last reward: -107.20\n",
      "iteration 475 - last reward: -70.39\n",
      "iteration 480 - last reward: -23.64\n",
      "iteration 485 - last reward: -180.97\n",
      "iteration 490 - last reward: -27.72\n",
      "iteration 495 - last reward: -56.10\n",
      "iteration 500 - last reward: -84.34\n",
      "iteration 505 - last reward: -58.86\n",
      "iteration 510 - last reward: -17.19\n",
      "iteration 515 - last reward: -112.04\n",
      "iteration 520 - last reward: -107.08\n",
      "iteration 525 - last reward: -108.77\n",
      "iteration 530 - last reward: -52.07\n",
      "iteration 535 - last reward: -75.60\n",
      "iteration 540 - last reward: -23.96\n",
      "iteration 545 - last reward: -148.42\n",
      "iteration 550 - last reward: -90.32\n",
      "iteration 555 - last reward: -89.08\n",
      "iteration 560 - last reward: -76.65\n",
      "iteration 565 - last reward: -7.53\n",
      "iteration 570 - last reward: -135.51\n",
      "iteration 575 - last reward: -99.86\n",
      "iteration 580 - last reward: -72.92\n",
      "iteration 585 - last reward: -97.72\n",
      "iteration 590 - last reward: -88.45\n",
      "iteration 595 - last reward: -35.60\n",
      "iteration 600 - last reward: -99.88\n",
      "iteration 605 - last reward: -124.12\n",
      "iteration 610 - last reward: 3.91\n",
      "iteration 615 - last reward: -31.20\n",
      "iteration 620 - last reward: -89.95\n",
      "iteration 625 - last reward: -49.01\n",
      "iteration 630 - last reward: -93.83\n",
      "iteration 635 - last reward: -233.59\n",
      "iteration 640 - last reward: -78.51\n",
      "iteration 645 - last reward: -107.99\n",
      "iteration 650 - last reward: -109.97\n",
      "iteration 655 - last reward: -87.55\n",
      "iteration 660 - last reward: -20.29\n",
      "iteration 665 - last reward: -35.38\n",
      "iteration 670 - last reward: -134.74\n",
      "iteration 675 - last reward: -121.04\n",
      "iteration 680 - last reward: -132.06\n",
      "iteration 685 - last reward: -151.32\n",
      "iteration 690 - last reward: -124.96\n",
      "iteration 695 - last reward: -6.11\n",
      "iteration 700 - last reward: -115.70\n",
      "iteration 705 - last reward: -98.90\n",
      "iteration 710 - last reward: -72.83\n",
      "iteration 715 - last reward: -135.58\n",
      "iteration 720 - last reward: 1.81\n",
      "iteration 725 - last reward: -132.17\n",
      "iteration 730 - last reward: 59.98\n",
      "iteration 735 - last reward: -95.48\n",
      "iteration 740 - last reward: -44.11\n",
      "iteration 745 - last reward: -31.58\n",
      "iteration 750 - last reward: -89.28\n",
      "iteration 755 - last reward: -85.40\n",
      "iteration 760 - last reward: -108.67\n",
      "iteration 765 - last reward: -38.41\n",
      "iteration 770 - last reward: -73.51\n",
      "iteration 775 - last reward: -100.05\n",
      "iteration 780 - last reward: -8.57\n",
      "iteration 785 - last reward: -163.61\n",
      "iteration 790 - last reward: -9.18\n",
      "iteration 795 - last reward: -83.41\n",
      "iteration 800 - last reward: -87.99\n",
      "iteration 805 - last reward: -94.15\n",
      "iteration 810 - last reward: -43.23\n",
      "iteration 815 - last reward: 19.14\n",
      "iteration 820 - last reward: -22.73\n",
      "iteration 825 - last reward: -2.97\n",
      "iteration 830 - last reward: 56.54\n",
      "iteration 835 - last reward: 3.72\n",
      "iteration 840 - last reward: -165.94\n",
      "iteration 845 - last reward: -73.36\n",
      "iteration 850 - last reward: -56.34\n",
      "iteration 855 - last reward: 2.81\n",
      "iteration 860 - last reward: 43.47\n",
      "iteration 865 - last reward: -33.02\n",
      "iteration 870 - last reward: -70.27\n",
      "iteration 875 - last reward: 24.88\n",
      "iteration 880 - last reward: -71.63\n",
      "iteration 885 - last reward: -157.72\n",
      "iteration 890 - last reward: -199.78\n",
      "iteration 895 - last reward: -45.82\n",
      "iteration 900 - last reward: -213.77\n",
      "iteration 905 - last reward: -24.29\n",
      "iteration 910 - last reward: 38.46\n",
      "iteration 915 - last reward: -26.59\n",
      "iteration 920 - last reward: -41.26\n",
      "iteration 925 - last reward: 42.17\n",
      "iteration 930 - last reward: -39.56\n",
      "iteration 935 - last reward: -83.88\n",
      "iteration 940 - last reward: -22.17\n",
      "iteration 945 - last reward: -20.61\n",
      "iteration 950 - last reward: -19.44\n",
      "iteration 955 - last reward: -74.76\n",
      "iteration 960 - last reward: -96.29\n",
      "iteration 965 - last reward: -23.59\n",
      "iteration 970 - last reward: -49.30\n",
      "iteration 975 - last reward: 51.91\n",
      "iteration 980 - last reward: -171.95\n",
      "iteration 985 - last reward: 76.89\n",
      "iteration 990 - last reward: 43.48\n",
      "iteration 995 - last reward: -53.84\n",
      "iteration 1000 - last reward: -62.92\n",
      "iteration 1005 - last reward: -83.80\n",
      "iteration 1010 - last reward: 88.07\n",
      "iteration 1015 - last reward: 34.50\n",
      "iteration 1020 - last reward: 117.32\n",
      "iteration 1025 - last reward: 66.04\n",
      "iteration 1030 - last reward: -36.93\n",
      "iteration 1035 - last reward: -40.46\n",
      "iteration 1040 - last reward: -41.06\n",
      "iteration 1045 - last reward: -31.72\n",
      "iteration 1050 - last reward: -8.91\n",
      "iteration 1055 - last reward: 4.20\n",
      "iteration 1060 - last reward: -48.63\n",
      "iteration 1065 - last reward: 15.80\n",
      "iteration 1070 - last reward: -18.94\n",
      "iteration 1075 - last reward: 46.89\n",
      "iteration 1080 - last reward: -37.82\n",
      "iteration 1085 - last reward: -54.52\n",
      "iteration 1090 - last reward: 42.17\n",
      "iteration 1095 - last reward: 9.25\n",
      "iteration 1100 - last reward: 13.46\n",
      "iteration 1105 - last reward: 1.65\n",
      "iteration 1110 - last reward: 90.14\n",
      "iteration 1115 - last reward: 22.93\n",
      "iteration 1120 - last reward: -60.94\n",
      "iteration 1125 - last reward: 118.81\n",
      "iteration 1130 - last reward: 29.04\n",
      "iteration 1135 - last reward: 35.92\n",
      "iteration 1140 - last reward: 121.26\n",
      "iteration 1145 - last reward: 85.83\n",
      "iteration 1150 - last reward: 57.73\n",
      "iteration 1155 - last reward: -35.34\n",
      "iteration 1160 - last reward: 37.96\n",
      "iteration 1165 - last reward: 129.12\n",
      "iteration 1170 - last reward: 118.53\n",
      "iteration 1175 - last reward: -19.32\n",
      "iteration 1180 - last reward: -157.26\n",
      "iteration 1185 - last reward: -38.42\n",
      "iteration 1190 - last reward: 114.75\n",
      "iteration 1195 - last reward: -264.47\n",
      "iteration 1200 - last reward: 181.66\n",
      "iteration 1205 - last reward: 151.23\n",
      "iteration 1210 - last reward: 16.78\n",
      "iteration 1215 - last reward: 18.72\n",
      "iteration 1220 - last reward: 186.88\n",
      "iteration 1225 - last reward: -42.50\n",
      "iteration 1230 - last reward: -18.59\n",
      "iteration 1235 - last reward: 62.47\n",
      "iteration 1240 - last reward: -205.84\n",
      "iteration 1245 - last reward: 98.01\n",
      "iteration 1250 - last reward: 15.01\n",
      "iteration 1255 - last reward: 166.79\n",
      "iteration 1260 - last reward: 86.62\n",
      "iteration 1265 - last reward: 141.26\n",
      "iteration 1270 - last reward: 46.07\n",
      "iteration 1275 - last reward: 8.48\n",
      "iteration 1280 - last reward: 34.71\n",
      "iteration 1285 - last reward: -54.23\n",
      "iteration 1290 - last reward: -24.48\n",
      "iteration 1295 - last reward: 84.50\n",
      "iteration 1300 - last reward: -78.08\n",
      "iteration 1305 - last reward: 53.48\n",
      "iteration 1310 - last reward: 91.94\n",
      "iteration 1315 - last reward: -13.00\n",
      "iteration 1320 - last reward: 93.41\n",
      "iteration 1325 - last reward: 27.31\n",
      "iteration 1330 - last reward: -33.23\n",
      "iteration 1335 - last reward: 124.00\n",
      "iteration 1340 - last reward: 32.66\n",
      "iteration 1345 - last reward: 3.74\n",
      "iteration 1350 - last reward: 122.82\n",
      "iteration 1355 - last reward: 88.45\n",
      "iteration 1360 - last reward: 87.71\n",
      "iteration 1365 - last reward: 123.05\n",
      "iteration 1370 - last reward: 10.00\n",
      "iteration 1375 - last reward: 77.19\n",
      "iteration 1380 - last reward: -38.85\n",
      "iteration 1385 - last reward: 4.31\n",
      "iteration 1390 - last reward: 44.01\n",
      "iteration 1395 - last reward: 159.59\n",
      "iteration 1400 - last reward: 189.30\n",
      "iteration 1405 - last reward: 201.51\n",
      "iteration 1410 - last reward: 83.95\n",
      "iteration 1415 - last reward: 102.82\n",
      "iteration 1420 - last reward: 166.31\n",
      "iteration 1425 - last reward: 24.32\n",
      "iteration 1430 - last reward: -0.40\n",
      "iteration 1435 - last reward: -25.99\n",
      "iteration 1440 - last reward: 117.60\n",
      "iteration 1445 - last reward: 16.34\n",
      "iteration 1450 - last reward: 136.00\n",
      "iteration 1455 - last reward: 9.37\n",
      "iteration 1460 - last reward: 26.65\n",
      "iteration 1465 - last reward: -5.06\n",
      "iteration 1470 - last reward: 63.99\n",
      "iteration 1475 - last reward: -142.79\n",
      "iteration 1480 - last reward: -10.47\n",
      "iteration 1485 - last reward: 139.47\n",
      "iteration 1490 - last reward: 53.00\n",
      "iteration 1495 - last reward: -21.23\n",
      "iteration 1500 - last reward: 147.35\n",
      "iteration 1505 - last reward: 262.10\n",
      "iteration 1510 - last reward: 58.64\n",
      "iteration 1515 - last reward: 234.61\n",
      "iteration 1520 - last reward: 188.07\n",
      "iteration 1525 - last reward: 255.02\n",
      "iteration 1530 - last reward: 135.98\n",
      "iteration 1535 - last reward: 53.71\n",
      "iteration 1540 - last reward: 266.70\n",
      "iteration 1545 - last reward: 160.37\n",
      "iteration 1550 - last reward: -41.38\n",
      "iteration 1555 - last reward: 208.11\n",
      "iteration 1560 - last reward: 63.52\n",
      "iteration 1565 - last reward: 18.06\n",
      "iteration 1570 - last reward: 268.27\n",
      "iteration 1575 - last reward: 224.65\n",
      "iteration 1580 - last reward: -21.45\n",
      "iteration 1585 - last reward: 277.06\n",
      "iteration 1590 - last reward: 209.71\n",
      "iteration 1595 - last reward: 3.22\n",
      "iteration 1600 - last reward: 254.35\n",
      "iteration 1605 - last reward: 156.46\n",
      "iteration 1610 - last reward: 91.20\n",
      "iteration 1615 - last reward: 184.38\n",
      "iteration 1620 - last reward: 235.25\n",
      "iteration 1625 - last reward: 29.13\n",
      "iteration 1630 - last reward: 209.42\n",
      "iteration 1635 - last reward: 125.06\n",
      "iteration 1640 - last reward: 130.90\n",
      "iteration 1645 - last reward: 221.54\n",
      "iteration 1650 - last reward: -136.78\n",
      "iteration 1655 - last reward: -34.69\n",
      "iteration 1660 - last reward: 268.83\n",
      "iteration 1665 - last reward: 24.68\n",
      "iteration 1670 - last reward: 53.02\n",
      "iteration 1675 - last reward: 125.39\n",
      "iteration 1680 - last reward: 165.09\n",
      "iteration 1685 - last reward: 145.83\n",
      "iteration 1690 - last reward: 156.34\n",
      "iteration 1695 - last reward: 222.19\n",
      "iteration 1700 - last reward: 109.88\n",
      "iteration 1705 - last reward: -152.26\n",
      "iteration 1710 - last reward: -169.80\n",
      "iteration 1715 - last reward: 220.43\n",
      "iteration 1720 - last reward: 110.21\n",
      "iteration 1725 - last reward: -171.45\n",
      "iteration 1730 - last reward: 108.10\n",
      "iteration 1735 - last reward: 268.77\n",
      "iteration 1740 - last reward: -206.85\n",
      "iteration 1745 - last reward: 186.76\n",
      "iteration 1750 - last reward: 238.76\n",
      "iteration 1755 - last reward: 135.68\n",
      "iteration 1760 - last reward: 233.55\n",
      "iteration 1765 - last reward: 247.81\n",
      "iteration 1770 - last reward: 282.37\n",
      "iteration 1775 - last reward: 293.12\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run deep Q-learning.\"\"\"\n",
    "\n",
    "# ---> TODO: find good hyperparameters\n",
    "discount_factor = 0.99 # gamma value\n",
    "learning_rate = 1e-4 # alpha value\n",
    "epsilon_start = 1.0 #\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "# Create environment and reset it\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "observation, info = env.reset(seed=None)\n",
    "\n",
    "# Create Q-network and enable train mode\n",
    "device = torch.device(\"cpu\")\n",
    "q_network = QNetwork().to(device)\n",
    "q_network.train()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "# Launch training\n",
    "running_reward = 0.0\n",
    "training_iteration = 0\n",
    "epsilon = epsilon_start\n",
    "while True:\n",
    "\n",
    "    # Reset the environment\n",
    "    observation, info = env.reset()\n",
    "    episode_total_reward = 0.0\n",
    "\n",
    "    # Sample a trajectory\n",
    "    while True:\n",
    "\n",
    "        # Epsilon-greedy action selection (random)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Epsilon-greedy action selection (best action)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Add batch dimension and transform to tensor\n",
    "                x = np.expand_dims(observation, 0)\n",
    "                x = torch.from_numpy(x).float().to(device)\n",
    "                q_values = q_network(x)\n",
    "\n",
    "                # ---> TODO: how to compute action (taking the action has max q_values)\n",
    "                # Select the action with the highest predicted Q-value.\n",
    "                # .argmax() returns the index of the maximum value, which corresponds to the best action.\n",
    "                # .item() extracts the value from the tensor.\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        # Take the action\n",
    "        observation_next, reward, terminated, truncated, info = env.step(\n",
    "            action)\n",
    "\n",
    "        # Check if episode is done and save reward\n",
    "        done = terminated or truncated\n",
    "        episode_total_reward += reward\n",
    "\n",
    "        # Compute the TD target\n",
    "        with torch.no_grad():\n",
    "            x_next = np.expand_dims(observation_next, 0)\n",
    "            x_next = torch.from_numpy(x_next).float().to(device)\n",
    "            q_next = q_network(x_next)\n",
    "            q_next_max = q_next.max(dim=1).values.item()\n",
    "\n",
    "            # ---> TODO: compute the TD target\n",
    "            # If the episode is done, there is no next state, so the target is just the immediate reward.\n",
    "            # Otherwise, the target is the reward plus the discounted value of the best action in the next state.\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + discount_factor * q_next_max\n",
    "\n",
    "        # TD prediction\n",
    "        x = np.expand_dims(observation, 0)\n",
    "        x = torch.from_numpy(x).float().to(device)\n",
    "        q_pred = q_network(x)[0, action]\n",
    "\n",
    "        # ---> TODO: compute loss and update\n",
    "        # diff between what we want and what we have\n",
    "        # The loss is the difference between our target (ideal value) and our prediction (current estimate).\n",
    "        # We use Mean Squared Error loss, which is standard for Q-learning.\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(q_pred, torch.tensor(target).to(device).float())\n",
    "\n",
    "        # Reset gradients to 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients of the loss (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the policy parameters (gradient ascent)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Transition\n",
    "        observation = observation_next\n",
    "\n",
    "        # End episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Logging\n",
    "    running_reward = 0.1 * episode_total_reward + 0.9 * running_reward\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    # Log results\n",
    "    log_frequency = 5\n",
    "    training_iteration += 1\n",
    "    if training_iteration % log_frequency == 0:\n",
    "\n",
    "        # Save neural network\n",
    "        torch.save(q_network.state_dict(), \"q_network.pt\")\n",
    "\n",
    "        # Print results\n",
    "        print(\"iteration {} - last reward: {:.2f}\".format(\n",
    "            training_iteration, episode_total_reward))\n",
    "\n",
    "        # Exit condition\n",
    "        if running_reward >= 200:\n",
    "            break\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70460079-2711-4e1e-b9b6-3c6e5758fcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward = 222.86713600510137\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test Q-network.\"\"\"\n",
    "\n",
    "# Create environment and reset it\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "observation, info = env.reset(seed=None)\n",
    "total_reward = 0.0\n",
    "\n",
    "# Load trained Q-network and enable test mode\n",
    "device = torch.device(\"cpu\")\n",
    "q_network = QNetwork().to(device)\n",
    "q_network.load_state_dict(torch.load(\"q_network.pt\", weights_only=True))\n",
    "q_network.eval()\n",
    "\n",
    "# While the episode is not finished\n",
    "finished = False\n",
    "while not finished:\n",
    "\n",
    "    # Add batch dimension and transform to tensor\n",
    "    x = np.expand_dims(observation, 0)\n",
    "    x = torch.from_numpy(x).float().to(device)\n",
    "\n",
    "    # Compute action from the Q-table\n",
    "    # action = q_network(x)\n",
    "    with torch.no_grad():\n",
    "        q_values_for_current_state = q_network(x)\n",
    "\n",
    "    # ---> TODO: how to select an action\n",
    "    # In evaluation, we always act greedily by choosing the action with the highest Q-value.\n",
    "    # No exploration (no epsilon-greedy) is needed here.\n",
    "    # action = q_values.argmax().item()\n",
    "    action = q_values_for_current_state.argmax().item()\n",
    "\n",
    "    # One step forward\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    finished = terminated or truncated\n",
    "\n",
    "    # Eventually render the environment (render mode should be \"human\")\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "# Print reward\n",
    "print(\"total_reward = {}\".format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d7b55-4e90-40ee-b686-988f0d39f78d",
   "metadata": {},
   "source": [
    "Through hyperparameter tuning, I configured the settings as follows: a **discount factor** of 0.99, a **learning rate** of 1e-4, an **epsilon_start** of 1.0, an **epsilon_end** of 0.01, and an **epsilon_decay** of 0.999. I also made a minor change to the action selection logic in the test script. After approximately 1900 training iterations, the final Q-network testing consistently yielded scores above 200 across multiple runs.\n",
    "\n",
    "**Significance of the Result**: A consistent score above 200 signifies that the agent has successfully \"solved\" the Lunar Lander environment. This result demonstrates that the agent has learned a robust and effective policy, going beyond simply avoiding a crash. It has mastered the complex control task of using its thrusters to gently slow its descent, maintain stability, and navigate precisely to the landing pad, thereby achieving the maximum rewards for a successful landing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cd582-0fc6-45ce-b50f-d90121b5402d",
   "metadata": {},
   "source": [
    "## 3) REINFORCE algorithm\n",
    "\n",
    "We want to build a policy $\\pi_\\theta(a | s) = P(a | s, \\theta)$ that gives the probability of choosing an action $a$ in state $s$.\n",
    "The policy is a deep neural network parameterized by some weights $\\theta$.\n",
    "The policy is also referred to as \"actor\".\n",
    "\n",
    "We want to find the parameters $\\theta$ that maximize the performance measure $J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[ G_0 ]$ with $G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$ and $\\gamma \\in [0, 1]$ being a discount factor.\n",
    "To do so, we use the gradient ascent method: $\\theta_{k+1} = \\theta_{k} + \\alpha \\nabla_{\\theta_k} J(\\theta_k)$ with $\\alpha$ being the learning rate.\n",
    "The performance measure depends on both the action selection and the distribution of states.\n",
    "Both are affected by the policy parameters, which make the computation of the gradient challenging.\n",
    "\n",
    "The policy gradient theorem gives an expression for $\\nabla_\\theta J(\\theta)$ that does not involve the derivative of the state distribution.\n",
    "The expectation is over all possible state-action trajectories over the policy $\\pi_\\theta$:\n",
    "$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[ \\sum_{t=0}^{\\infty} G_t \\nabla_\\theta \\ln \\pi_\\theta(a_t | s_t) ]$.\n",
    "In the REINFORCE algorithm, we use a Monte-Carlo estimate over one episode, i.e., one trajectory:\n",
    "$\\nabla_\\theta J(\\theta) = \\sum_{t=0}^{\\infty} G_t \\nabla_\\theta \\ln \\pi_\\theta(a_t | s_t)$.\n",
    "\n",
    "Your objective is to complete the REINFORCE algorithm to train the policy until convergence. To solve the problem, you need to achieve a cumulative reward of at least 200 when training the policy. Below are: the code of the REINFORCE algorithm and a script to test your policy once it is trained.\n",
    "\n",
    "Below are 3 code samples.\n",
    "- A class implementing the policy. You must specify `input_size` and `nb_actions`.\n",
    "- A script to train the policy using REINFORCE. Complete the missing parts of the code.\n",
    "- A script to test the policy. Determine how to select an action from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4fd9d6-a246-4c3f-8a78-432f587e380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(nn.Module):\n",
    "    \"\"\"Deep neural network policy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(ActorModel, self).__init__()\n",
    "        # --> TODO: specify the correct input and output sizes\n",
    "        input_size = 0\n",
    "        nb_actions = 0\n",
    "\n",
    "        # Layers\n",
    "        self.layer_a = nn.Linear(input_size, 128)\n",
    "        self.layer_b = nn.Linear(128, 128)\n",
    "        self.policy = nn.Linear(128, nb_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward.\"\"\"\n",
    "        x = F.relu(self.layer_a(x))\n",
    "        x = F.relu(self.layer_b(x))\n",
    "        action_prob = F.softmax(self.policy(x), dim=-1)\n",
    "        return action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d812eb-aa0d-4890-a34a-863967836e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run REINFORCE.\"\"\"\n",
    "\n",
    "# ---> TODO: find good hyperparameters\n",
    "discount_factor = 1.0\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Create environment and reset it\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "obsevation, info = env.reset(seed=None)\n",
    "\n",
    "# Create policy and enable train mode\n",
    "device = torch.device(\"cpu\")\n",
    "policy = ActorModel().to(device)\n",
    "policy.train()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# Launch training\n",
    "running_reward = 0.0\n",
    "training_iteration = 0\n",
    "while True:\n",
    "\n",
    "    # Experience\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # Reset the environment\n",
    "    obsevation, info = env.reset()\n",
    "\n",
    "    # During experience, we will save:\n",
    "    # - the probability of the chosen action at each time step pi(at|st)\n",
    "    # - the rewards received at each time step ri\n",
    "    saved_probabilities = list()\n",
    "    saved_rewards = list()\n",
    "\n",
    "    # Sample a trajectory\n",
    "    while True:\n",
    "\n",
    "        # Add batch dimension and transform to tensor\n",
    "        x = torch.from_numpy(np.expand_dims(obsevation, 0)).float()\n",
    "\n",
    "        # Create a categorical distribution over the list of probabilities\n",
    "        # of actions (given by the policy) and sample an action from it\n",
    "        probabilities = policy(x.to(device))\n",
    "        distribution = Categorical(probabilities)\n",
    "        action = distribution.sample()\n",
    "\n",
    "        # Take the action\n",
    "        obsevation, reward, terminated, truncated, info = env.step(\n",
    "            action.item())\n",
    "\n",
    "        # Save the probability of the chosen action and the reward\n",
    "        saved_probabilities.append(probabilities[0][action])\n",
    "        saved_rewards.append(reward)\n",
    "\n",
    "        # End episode\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Compute discounted sum of rewards\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # Current discounted reward\n",
    "    discounted_reward = 0.0\n",
    "\n",
    "    # List of all the discounted rewards, for each time step\n",
    "    discounted_rewards = list()\n",
    "\n",
    "    # ---> TODO: compute discounted rewards\n",
    "    for r in saved_rewards[::-1]:\n",
    "        pass\n",
    "\n",
    "    # Eventually normalize for stability purposes\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    mean, std = discounted_rewards.mean(), discounted_rewards.std()\n",
    "    discounted_rewards = (discounted_rewards - mean) / (std + 1e-7)\n",
    "\n",
    "    # Update policy parameters\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # For each time step\n",
    "    actor_loss = list()\n",
    "    for p, g in zip(saved_probabilities, discounted_rewards):\n",
    "\n",
    "        # ---> TODO: compute policy loss\n",
    "        time_step_actor_loss = 0\n",
    "\n",
    "        # Save it\n",
    "        actor_loss.append(time_step_actor_loss)\n",
    "\n",
    "    # Sum all the time step losses\n",
    "    actor_loss = torch.cat(actor_loss).sum()\n",
    "\n",
    "    # Reset gradients to 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the gradients of the loss (backpropagation)\n",
    "    actor_loss.backward()\n",
    "\n",
    "    # Update the policy parameters (gradient ascent)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # Episode total reward\n",
    "    episode_total_reward = sum(saved_rewards)\n",
    "    running_reward = 0.1 * episode_total_reward + 0.9 * running_reward\n",
    "\n",
    "    # Log results\n",
    "    log_frequency = 5\n",
    "    training_iteration += 1\n",
    "    if training_iteration % log_frequency == 0:\n",
    "\n",
    "        # Save neural network\n",
    "        torch.save(policy.state_dict(), \"policy.pt\")\n",
    "\n",
    "        # Print results\n",
    "        print(\"iteration {} - last reward: {:.2f}\".format(\n",
    "            training_iteration, episode_total_reward))\n",
    "\n",
    "        # Exit condition\n",
    "        if running_reward >= 200:\n",
    "            break\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbf7d4-88b1-44cd-b61c-7438cb2a5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test policy.\"\"\"\n",
    "# Create environment and reset it\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "obsevation, info = env.reset(seed=None)\n",
    "total_reward = 0.0\n",
    "\n",
    "# Load trained policy and enable test mode\n",
    "device = torch.device(\"cpu\")\n",
    "policy = ActorModel().to(device)\n",
    "policy.load_state_dict(torch.load(\"policy.pt\", weights_only=True))\n",
    "policy.eval()\n",
    "\n",
    "# While the episode is not finished\n",
    "finished = False\n",
    "while not finished:\n",
    "\n",
    "    # Add batch dimension and transform to tensor\n",
    "    x = torch.from_numpy(np.expand_dims(obsevation, 0)).float()\n",
    "\n",
    "    # Compute action from the policy\n",
    "    action = policy(x.to(device))\n",
    "\n",
    "    # ---> TODO: how to select an action\n",
    "    action = 0\n",
    "\n",
    "    # One step forward\n",
    "    obsevation, reward, terminated, truncated, info = env.step(action)\n",
    "    finished = terminated or truncated\n",
    "\n",
    "    # Eventually render the environment (render mode should be \"human\")\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "# Print reward\n",
    "print(\"total_reward = {}\".format(total_reward))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
